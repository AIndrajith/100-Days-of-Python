{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Wikipedia Article Details ---\n",
      "\n",
      "Title: Python (programming language)\n",
      "\n",
      "Summary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[33]\n",
      "\n",
      "Headings:\n",
      "- Contents\n",
      "- History\n",
      "- Design philosophy and features\n",
      "- Syntax and semantics\n",
      "- Indentation\n",
      "\n",
      "Related Links:\n",
      "- https://en.wikipedia.org/wiki/Code_readability\n",
      "- https://en.wikipedia.org/wiki/Boo_(programming_language)\n",
      "- https://en.wikipedia.org/wiki/There_is_more_than_one_way_to_do_it\n",
      "- https://en.wikipedia.org/wiki/Object_(computer_science)\n",
      "- https://en.wikipedia.org/wiki/CERN\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia Article Scraper\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Get Wikipedia Article URL\n",
    "def get_wikipedia_page(topic):\n",
    "  url = f\"https://en.wikipedia.org/wiki/{topic.replace(' ', '_')}\"\n",
    "  response = requests.get(url)\n",
    "  if response.status_code == 200:\n",
    "    return response.text\n",
    "  else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}. Check the topic and try again\")\n",
    "    return None\n",
    "\n",
    "# Step 2: Extract Article Title\n",
    "def get_article_title(soup):\n",
    "  return soup.find('h1').text\n",
    "\n",
    "# Step 3: Extract Article Summary\n",
    "def get_article_summary(soup):\n",
    "  paragraphs = soup.find_all('p')\n",
    "  for para in paragraphs:\n",
    "    if para.text.strip():\n",
    "      return para.text.strip()\n",
    "  return \"No summary found\"\n",
    "\n",
    "# Step 4: Extract Headings\n",
    "def get_headings(soup):\n",
    "  headings = [heading.text.strip() for heading in soup.find_all(['h2', 'h3', 'h4'])]\n",
    "  return headings\n",
    "\n",
    "# Step 5: Extract Related Links\n",
    "def get_related_links(soup):\n",
    "  links = []\n",
    "  for a_tag in soup.find_all('a', href=True):\n",
    "    href = a_tag['href']\n",
    "    if href.startswith('/wiki/') and \":\" not in href:\n",
    "      links.append(f\"https://en.wikipedia.org{href}\")\n",
    "  return list(set(links))[:5]\n",
    "\n",
    "# Step 6: Main Program\n",
    "def main():\n",
    "  topic = input(\"Enter a topic to search on Wikipedia: \").strip()\n",
    "  page_content = get_wikipedia_page(topic)\n",
    "\n",
    "  if page_content:\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    title = get_article_title(soup)\n",
    "    summary = get_article_summary(soup)\n",
    "    headings = get_headings(soup)\n",
    "    related_links = get_related_links(soup)\n",
    "\n",
    "    print(\"\\n--- Wikipedia Article Details ---\")\n",
    "    print(f\"\\nTitle: {title}\")\n",
    "    print(f\"\\nSummary: {summary}\")\n",
    "    print(\"\\nHeadings:\")\n",
    "    for heading in headings[:5]:\n",
    "      print(f\"- {heading}\")\n",
    "\n",
    "    print(\"\\nRelated Links:\")\n",
    "    for link in related_links:\n",
    "      print(f\"- {link}\")\n",
    "\n",
    "# Run Program\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
